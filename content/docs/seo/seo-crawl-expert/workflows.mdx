---
title: Workflows
description: Processus detailles et scenarios d'audit pour l'agent SEO Crawl Expert - workflows complets pour tous types d'audits SEO techniques.
---

# WORKFLOWS.md - SEO Crawl Expert

Processus detailles et scenarios d'audit pour l'agent seo-crawl-expert.

## Workflow 1 : Audit Technique Complet

### Contexte
Audit exhaustif d'un site web pour identifier tous les problemes de crawlabilite, indexation et performance technique.

### Prerequis
- Acces Google Search Console (proprietaire ou delegue)
- Outil de crawl configure (Screaming Frog, Sitebulb ou equivalent)
- Optionnel : acces logs serveur pour analyse approfondie

### Etapes

#### 1.1 Configuration Initiale (15-30 min)

1. **Identifier le perimetre**
   - Domaine principal et sous-domaines a inclure
   - Estimation du nombre de pages
   - Frequence de mise a jour du contenu

2. **Configurer le crawler**
   - User-agent : Googlebot ou crawler standard
   - Respect du robots.txt : active
   - Rendu JavaScript : active si site dynamique
   - Profondeur : illimitee ou selon taille

3. **Lancer un crawl de test**
   - Limiter a 100-500 pages
   - Verifier que toutes les sections sont accessibles
   - Ajuster la configuration si necessaire

#### 1.2 Crawl Complet (variable selon taille)

1. **Executer le crawl complet**
   - Surveiller la progression
   - Identifier les erreurs de crawl en temps reel

2. **Exporter les donnees**
   - Liste complete des URLs
   - Codes de statut
   - Redirections
   - Balises meta (title, description, robots)
   - Donnees structurees detectees

#### 1.3 Analyse d'Accessibilite (1-2h)

1. **Audit robots.txt**
   - Telecharger et analyser /robots.txt
   - Verifier les directives Disallow
   - Confirmer la presence du lien sitemap
   - Identifier les blocages non intentionnels

2. **Audit sitemaps**
   - Valider la structure XML
   - Comparer URLs sitemap vs URLs crawlees
   - Identifier les URLs 4xx/5xx dans le sitemap
   - Verifier la coherence avec les canonicals

3. **Verifier les meta robots**
   - Lister les pages avec noindex
   - Identifier les conflits (noindex + canonical externe)
   - Verifier les X-Robots-Tag HTTP

#### 1.4 Analyse de Crawlabilite (1-2h)

1. **Evaluer la profondeur de crawl**
   - Exporter le rapport de profondeur
   - Identifier les pages > 3 clics de la homepage
   - Prioriser les pages strategiques enterrees

2. **Analyser les redirections**
   - Lister toutes les redirections 3xx
   - Identifier les chaines > 2 sauts
   - Reperer les redirections en boucle
   - Quantifier les redirections internes

3. **Detecter le contenu duplique**
   - Pages avec meme title/description
   - Pages avec contenu similaire (si disponible)
   - Verifier les parametres d'URL (tri, filtres)

#### 1.5 Analyse Pages Orphelines (30 min - 1h)

1. **Exporter les URLs depuis GSC**
   - Rapport Couverture > Pages indexees
   - Comparer avec les URLs crawlees

2. **Identifier les orphelines**
   - URLs indexees mais non crawlees = potentielles orphelines
   - Verifier manuellement un echantillon

3. **Evaluer l'impact**
   - Traffic organique des pages orphelines
   - Importance strategique

#### 1.6 Analyse Maillage Interne (1-2h)

1. **Cartographier le link equity**
   - Pages avec le plus de liens internes (hub pages)
   - Pages avec le moins de liens (sous-liees)
   - Distribution du PageRank interne

2. **Analyser les ancres**
   - Diversite des ancres
   - Ancres sur-optimisees
   - Ancres generiques ("cliquez ici")

3. **Identifier les opportunites**
   - Liens contextuels manquants
   - Pages strategiques sous-liees

#### 1.7 Analyse Technique (1-2h)

1. **Evaluer le rendu JavaScript**
   - Comparer HTML initial vs DOM rendu
   - Identifier le contenu dependant du JS
   - Verifier les erreurs console

2. **Auditer Core Web Vitals**
   - Rapport PageSpeed Insights pages cles
   - Rapport CrUX depuis GSC
   - Identifier les pages problematiques

3. **Valider les donnees structurees**
   - Tester un echantillon via Rich Results Test
   - Identifier les erreurs/warnings
   - Verifier les types implementes

#### 1.8 Synthese et Rapport (1-2h)

1. **Classifier les problemes**
   - Critique / Important / Modere / Opportunite
   - Impact estime sur le SEO

2. **Prioriser les actions**
   - Quick wins (impact eleve, effort faible)
   - Projets majeurs (impact eleve, effort important)

3. **Rediger le rapport**
   - Executive summary
   - Findings detailles avec captures
   - Plan d'action priorise

---

## Workflow 2 : Audit Rapide Post-Migration

### Contexte
Verification urgente apres migration de site, refonte ou changement de CMS.

### Prerequis
- URLs de l'ancien site (sitemap, crawl anterieur)
- Acces au nouveau site en production

### Etapes (2-4h total)

#### 2.1 Verification Immediate (30 min)

1. **Tester l'accessibilite**
   - Homepage charge correctement
   - robots.txt accessible et non bloquant
   - sitemap.xml present et valide

2. **Verifier les pages critiques**
   - 10-20 pages strategiques
   - Codes 200, contenu present
   - Pas de noindex accidentel

#### 2.2 Audit Redirections (1-2h)

1. **Mapper ancien â†’ nouveau**
   - Importer les URLs de l'ancien sitemap
   - Crawler en mode liste
   - Verifier chaque redirection

2. **Identifier les problemes**
   - 404 (redirections manquantes)
   - Chaines de redirections
   - Redirections vers mauvaise destination

3. **Prioriser par trafic**
   - Croiser avec donnees Analytics
   - Corriger d'abord les pages a fort trafic

#### 2.3 Verification Canonicals (30 min)

1. **Sample check**
   - Verifier les canonicals sur pages types
   - S'assurer qu'ils pointent vers nouvelles URLs
   - Pas de canonical vers ancien domaine

#### 2.4 Soumission GSC (15 min)

1. **Actions immediates**
   - Soumettre nouveau sitemap
   - Demander indexation pages prioritaires
   - Surveiller le rapport Couverture

---

## Workflow 3 : Diagnostic Probleme d'Indexation

### Contexte
Pages qui ne s'indexent pas ou disparaissent de l'index Google.

### Etapes (1-2h)

#### 3.1 Investigation Initiale (15 min)

1. **Verifier dans GSC**
   - URL Inspection tool sur la page
   - Statut d'indexation
   - Derniere date de crawl

2. **Rechercher la page**
   - site:domain.com/path exact
   - La page apparait-elle ?

#### 3.2 Analyse Technique (30-45 min)

1. **Verifier l'accessibilite**
   - Code de statut HTTP (doit etre 200)
   - Pas de blocage robots.txt
   - Pas de noindex

2. **Verifier les signaux**
   - Canonical correct (self-referencing ou absent)
   - Pas de redirect loop
   - Contenu suffisant et unique

3. **Evaluer la qualite**
   - Contenu thin ou duplique ?
   - Liens internes pointant vers la page ?
   - Page orpheline ?

#### 3.3 Actions Correctives (30 min)

1. **Corriger les blocages**
   - Retirer noindex si errone
   - Corriger le canonical
   - Ajouter des liens internes

2. **Forcer le recrawl**
   - Soumettre via URL Inspection
   - Demander indexation

3. **Documenter et suivre**
   - Noter les corrections
   - Planifier un check a J+7

---

## Workflow 4 : Optimisation Crawl Budget

### Contexte
Site volumineux (>50k pages) avec problemes de crawl budget identifies.

### Prerequis
- Acces logs serveur
- Outil d'analyse de logs (Screaming Frog Log Analyzer, Splunk)

### Etapes (4-8h)

#### 4.1 Analyse des Logs (2-3h)

1. **Importer les logs**
   - Minimum 30 jours de donnees
   - Filtrer sur Googlebot

2. **Identifier les patterns**
   - Pages les plus crawlees
   - Pages jamais crawlees
   - Frequence de crawl par section

3. **Detecter les anomalies**
   - Crawl excessif sur pages parametrees
   - Sections strategiques sous-crawlees
   - Pics ou chutes de crawl

#### 4.2 Diagnostic (1-2h)

1. **Classifier les URLs**
   - Indexables de valeur
   - Indexables mais faible valeur
   - Non-indexables (bloquees, noindex)

2. **Calculer le gaspillage**
   - % de crawl sur pages non-indexables
   - % de crawl sur contenu duplique
   - % de crawl sur parametres

#### 4.3 Plan d'Optimisation (1-2h)

1. **Prioriser les actions**
   - Bloquer les URLs parasites via robots.txt
   - Implementer parameter handling dans GSC
   - Ajouter noindex sur pages faible valeur

2. **Restructurer si necessaire**
   - Consolider le contenu thin
   - Ameliorer le maillage vers pages strategiques

3. **Monitoring**
   - Mettre en place un suivi mensuel des logs
   - Alertes sur variations anormales

---

## Workflow 5 : Audit Core Web Vitals

### Contexte
Ameliorer les scores Core Web Vitals pour impact SEO et UX.

### Etapes (2-4h)

#### 5.1 Etat des Lieux (30 min)

1. **Donnees de terrain (Field Data)**
   - Rapport GSC > Core Web Vitals
   - Donnees CrUX par groupe de pages

2. **Donnees de lab**
   - PageSpeed Insights sur 5-10 pages types
   - WebPageTest pour analyse detaillee

#### 5.2 Analyse LCP (45 min)

1. **Identifier l'element LCP**
   - Image hero ? Texte ? Video ?
   - Taille et format de l'element

2. **Diagnostiquer les causes**
   - Temps serveur (TTFB)
   - Ressources bloquantes
   - Images non optimisees
   - Lazy loading excessif

#### 5.3 Analyse INP (45 min)

1. **Identifier les interactions lentes**
   - Chrome DevTools > Performance
   - Interactions > 200ms

2. **Diagnostiquer les causes**
   - JavaScript lourd sur le main thread
   - Event handlers mal optimises
   - Third-party scripts bloquants

#### 5.4 Analyse CLS (30 min)

1. **Identifier les shifts**
   - Layout Shift Regions dans DevTools
   - Elements sans dimensions reservees

2. **Causes communes**
   - Images sans width/height
   - Fonts web (FOUT/FOIT)
   - Ads et embeds dynamiques

#### 5.5 Recommandations (30 min)

1. **Quick wins**
   - Preload ressources critiques
   - Ajouter dimensions aux images
   - Lazy load below-the-fold

2. **Optimisations avancees**
   - CDN et edge caching
   - Code splitting
   - Service workers

---

## Workflow 6 : Validation Donnees Structurees

### Contexte
Implementer ou auditer le JSON-LD Schema.org d'un site.

### Etapes (1-3h selon complexite)

#### 6.1 Inventaire (30 min)

1. **Lister les types de pages**
   - Homepage
   - Pages produits
   - Articles/Blog
   - Pages contact/FAQ

2. **Mapper les schemas appropries**
   - WebSite + Organization pour homepage
   - Product pour e-commerce
   - Article pour blog
   - FAQPage pour FAQ

#### 6.2 Validation Existant (30-60 min)

1. **Crawler avec extraction schema**
   - Screaming Frog > Configuration > Extraction > Schema.org

2. **Tester un echantillon**
   - 2-3 pages par type dans Rich Results Test
   - Noter erreurs et warnings

#### 6.3 Corrections (30-60 min)

1. **Erreurs critiques**
   - Champs requis manquants
   - Types incorrects
   - URLs invalides

2. **Warnings**
   - Champs recommandes
   - Amelioration eligibilite rich results

#### 6.4 Implementation Nouvelles Schemas (variable)

1. **Rediger le JSON-LD**
   - Utiliser generateurs pour base
   - Adapter aux donnees reelles

2. **Tester avant deploiement**
   - Schema.org Validator pour syntaxe
   - Rich Results Test pour eligibilite

3. **Deployer et monitorer**
   - Verifier dans GSC > Ameliorations
   - Surveiller l'apparition de rich results

---

## Points de Decision

### Quand escalader ?

| Situation | Action |
|-----------|--------|
| Blocage complet d'indexation | Escalade immediate, impact business |
| Migration avec >5% de 404 | Alerter, correction prioritaire |
| Core Web Vitals "Poor" | Planifier optimisation, modere |
| Orphan pages < 1% du site | Opportunite, priorite basse |

### Quand recommander des outils payants ?

- Site > 500 URLs : Screaming Frog licence recommandee
- Site > 50k URLs : Solution enterprise (Lumar, Botify)
- Besoin de rapports clients : Sitebulb ou Semrush
- Analyse logs reguliere : Log analyzer dedie

### Quand faire appel a un developpeur ?

- Implementation SSR/SSG pour site SPA
- Corrections Core Web Vitals complexes (refactoring JS)
- Mise en place de donnees structurees dynamiques
- Configuration serveur (compression, cache, CDN)
